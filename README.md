# ğŸ•·ï¸ Web Crawler

A lightweight Python-based web crawler designed to crawl websites, extract URLs, and collect structured page content efficiently. This project was built for learning and exploring how basic crawling, content parsing, and recursive link exploration works.

---

## ğŸš€ Features

- Crawl and extract links from a target domain
- Parse page content using BeautifulSoup
- Avoids duplicate URL visits using a set
- Recursively crawls discovered links within the same domain
- Customizable depth level for crawling
- CLI-friendly â€” easily run from the terminal

---

## ğŸ› ï¸ Tech Stack

- **Python 3**
- **Requests** â€“ For making HTTP requests
- **BeautifulSoup4** â€“ For HTML parsing
- **re (Regex)** â€“ For URL pattern matching and cleanup

---

## ğŸ“ Project Structure


web_crawler/
â”‚
â”œâ”€â”€ main.py # Entry point for crawling
â”œâ”€â”€ crawler # Core crawling logic
â”œâ”€â”€ utils # Helper functions
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # Project documentation

---

## âš™ï¸ How It Works

1. Starts crawling from a given root URL.
2. Extracts all internal links using anchor tags (`<a href="">`).
3. Normalizes and filters links to avoid external sites.
4. Avoids revisiting the same URLs.
5. Recursively crawls links up to a specified depth.

---

## âœ… Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/Ankit1017/web_crawler.git
cd web_crawler

ğŸ“š Use Cases
SEO and internal link analysis

Learning web scraping and crawling

Research and content discovery

Custom website mapping tools

ğŸ§  Future Enhancements
Add asyncio or multithreading

Support robots.txt and rate limiting

Export results to CSV/JSON

Extract content like headings, metadata, or paragraphs

ğŸ™‹â€â™‚ï¸ Author
Ankit Bansal
GitHub: @Ankit1017

ğŸ“„ License
This project is licensed under the MIT License.
